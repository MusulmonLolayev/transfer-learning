{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect and convert results to Latex tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'mobilenet_v3-cifar10.txt'.lower()\n",
    "num_exams = \"Number of examples in computing weights heuristically: \".lower()\n",
    "error_msg = \"A error occured\".lower()\n",
    "not_found = \"not found from\".lower()\n",
    "\n",
    "model_version = \"Model version:\".lower()\n",
    "\n",
    "def empty_res_obj():\n",
    "  return {\n",
    "    'subset_size': [],\n",
    "  'h_train_loss': [],\n",
    "  'h_train_acc': [],\n",
    "  'h_test_loss': [],\n",
    "  'h_test_acc': [],\n",
    "  'gh_train_loss': [],\n",
    "  'gh_train_acc': [],\n",
    "  'gh_test_loss': [],\n",
    "  'gh_test_acc': [],\n",
    "  'g_train_loss': [],\n",
    "  'g_train_acc': [],\n",
    "  'g_test_loss': [],\n",
    "  'g_test_acc': []}\n",
    "\n",
    "def extract_values(line: str):\n",
    "  arrs = line.split(' ')\n",
    "  # [:-1] dropping ','\n",
    "  return float(arrs[2][:-1]), float(arrs[4])\n",
    "\n",
    "def collect_results(path):\n",
    "  all_res = {}\n",
    "  with open(path, 'r') as f:\n",
    "    for line in f:\n",
    "      line = line.lower()\n",
    "      if model_version not in line: continue\n",
    "      \n",
    "      # get model version\n",
    "      m_v = line[line.rfind(' ')+1:-1]\n",
    "      res = empty_res_obj()\n",
    "      all_res[m_v] = res\n",
    "      for size in [64, 128, 256, 512, 1024, 2048]:\n",
    "        # skip empty line\n",
    "        f.readline()\n",
    "        # get the subset size\n",
    "        line = f.readline().lower()\n",
    "        set_size = int(line[line.rfind(' ')+1:])\n",
    "        res['subset_size'].append(set_size)\n",
    "        # skips: shape\n",
    "        f.readline()\n",
    "        # heuristic info\n",
    "        f.readline()\n",
    "\n",
    "        line = f.readline().lower()\n",
    "        # check is whether error or info\n",
    "        if error_msg in line: continue\n",
    "        line = f.readline().lower()\n",
    "        if not_found in line: continue\n",
    "\n",
    "        # get results: heuristics on train\n",
    "        loss, acc = extract_values(line)\n",
    "        res['h_train_acc'].append(acc)\n",
    "        res['h_train_loss'].append(loss)\n",
    "\n",
    "        # get results: heuristics on test\n",
    "        loss, acc = extract_values(f.readline())\n",
    "        res['h_test_acc'].append(acc)\n",
    "        res['h_test_loss'].append(loss)\n",
    "\n",
    "        # skip the next info line:\n",
    "        # Training model on all heurstic weights with epochs 5\n",
    "        f.readline()\n",
    "        # get results: train on the heuristic weights\n",
    "        line = f.readline()\n",
    "        loss, acc = extract_values(line)\n",
    "        res['gh_train_acc'].append(acc)\n",
    "        res['gh_train_loss'].append(loss)\n",
    "\n",
    "        # get results: train on the heuristic weights\n",
    "        loss, acc = extract_values(f.readline())\n",
    "        res['gh_test_acc'].append(acc)\n",
    "        res['gh_test_loss'].append(loss)\n",
    "\n",
    "        # skip the next info line:\n",
    "        # Training model on all initial weights with epochs 5\n",
    "        f.readline()\n",
    "        # get results: train on the initial weights\n",
    "        loss, acc = extract_values(f.readline())\n",
    "        res['g_train_acc'].append(acc)\n",
    "        res['g_train_loss'].append(loss)\n",
    "\n",
    "        # get results: train on the initial weights\n",
    "        loss, acc = extract_values(f.readline())\n",
    "        res['g_test_acc'].append(acc)\n",
    "        res['g_test_loss'].append(loss)\n",
    "  \n",
    "  return all_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inceptionv3(cl.) & 0.15/0.16 & 0.14/0.13 & 0.16/0.13 & 0.17/0.16 & 0.18/0.15 & 0.21/0.20 \\\\\n",
      "inceptionv3(fe.) & 0.14/0.14 & 0.14/0.13 & 0.14/0.13 & 0.17/0.15 & 0.15/0.13 & 0.15/0.14 \\\\\n",
      "convnext(base-1k-224) & 0.14/0.14 & 0.25/0.25 & 0.20/0.20 & 0.13/0.14 & 0.15/0.17 & 0.19/0.18 \\\\\n",
      "resnet50(cl.) & 0.15/0.14 & 0.14/0.15 & 0.16/0.14 & 0.18/0.17 & 0.17/0.16 & 0.16/0.14 \\\\\n",
      "resnet50(fe.) & 0.13/0.14 & 0.13/0.15 & 0.16/0.16 & 0.13/0.12 & 0.18/0.18 & 0.15/0.15 \\\\\n",
      "efficientnet(b0-cl.) & 0.14/0.14 & 0.17/0.18 & 0.17/0.18 & 0.18/0.17 & 0.18/0.17 & 0.21/0.20 \\\\\n",
      "efficientnet(b0-fe.) & 0.16/0.15 & 0.18/0.16 & 0.19/0.19 & 0.22/0.20 & 0.19/0.18 & 0.20/0.19 \\\\\n",
      "efficientnet(b1-cl.) & 0.12/0.12 & 0.12/0.13 & 0.12/0.11 & 0.13/0.13 & 0.13/0.13 & 0.13/0.12 \\\\\n",
      "efficientnet(b1-fe.) & 0.13/0.12 & 0.14/0.13 & 0.14/0.12 & 0.13/0.13 & 0.14/0.13 & 0.15/0.13 \\\\\n",
      "efficientnet(b4-cl.) & 0.15/0.14 & 0.13/0.14 & 0.13/0.12 & 0.15/0.15 & 0.17/0.15 & 0.16/0.16 \\\\\n",
      "efficientnet(b4-fe.) & 0.13/0.15 & 0.19/0.19 & 0.15/0.15 & 0.17/0.17 & 0.19/0.18 & 0.20/0.17 \\\\\n",
      "mlp-mixer(mixer-b16-i1k-cl.) & 0.12/0.13 & 0.12/0.13 & 0.13/0.12 & 0.13/0.12 & 0.12/0.13 & 0.13/0.12 \\\\\n",
      "mlp-mixer(mixer-b16-i1k-fe.) & 0.17/0.16 & 0.13/0.12 & 0.21/0.21 & 0.20/0.19 & 0.22/0.23 & 0.25/0.26 \\\\\n",
      "mlp-mixer(mixer-b32-sam-cl.) & 0.12/0.14 & 0.12/0.14 & 0.13/0.15 & 0.12/0.13 & 0.12/0.13 & 0.12/0.13 \\\\\n",
      "mlp-mixer(mixer-b32-sam-fe.) & 0.27/0.26 & 0.22/0.24 & 0.30/0.28 & 0.33/0.30 & 0.37/0.37 & 0.36/0.36 \\\\\n",
      "visiontransformer(vit-b16-cl.) & 0.15/0.14 & 0.15/0.14 & 0.17/0.17 & 0.26/0.23 & 0.24/0.22 & 0.26/0.22 \\\\\n",
      "visiontransformer(vit-b16-fe.) & 0.20/0.19 & 0.20/0.18 & 0.29/0.26 & 0.26/0.25 & 0.30/0.27 & 0.33/0.30 \\\\\n",
      "mobilenetv3(large-075-224-cl.) & 0.16/0.17 & 0.16/0.14 & 0.16/0.14 & 0.18/0.16 & 0.17/0.17 & 0.20/0.17 \\\\\n",
      "mobilenetv3(small-075-224-cl.) & 0.20/0.19 & 0.22/0.19 & 0.18/0.19 & 0.25/0.22 & 0.27/0.26 & 0.26/0.25 \\\\\n",
      "mobilenetv3(small-075-224-fe.) & 0.19/0.18 & 0.23/0.19 & 0.23/0.23 & 0.20/0.21 & 0.29/0.29 & 0.24/0.24 \\\\\n"
     ]
    }
   ],
   "source": [
    "ds_name = 'mini_speech_commands'\n",
    "all_files = glob.glob(f'*{ds_name}.txt')\n",
    "\n",
    "for file in all_files:\n",
    "  path = f'./{file}'\n",
    "  all_res = collect_results(path)\n",
    "  m_name = file.replace(f'{ds_name}.txt', '')[:-1]\n",
    "  m_name = m_name.replace('_', '')\n",
    "  for m_v, res in all_res.items():\n",
    "    m_v = m_v.replace('classification', 'cl.')\\\n",
    "      .replace('feature-vector', 'fe.')\\\n",
    "      .replace('-fe', '-fe.')\\\n",
    "      .replace('..', '.')\n",
    "    print(f'{m_name}({m_v})', end='')\n",
    "    for tr_acc, te_acc in zip(res['h_train_acc'], res['h_test_acc']):\n",
    "      print(f\" & {tr_acc:.2f}/{te_acc:.2f}\", end='')\n",
    "    print(' \\\\\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1 12 13]\n",
      " [ 4  5  6]\n",
      " [ 7 18 19]]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 2, 3],\n",
    "                [4, 5, 6],\n",
    "                [7, 8, 9]])\n",
    "\n",
    "row_indices = np.array([0, 2])\n",
    "col_indices = np.array([1, 2])\n",
    "\n",
    "arr[np.ix_(row_indices, col_indices)] += 10\n",
    "\n",
    "print(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "rows = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
    "cols = np.arange(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[np.ix_(rows, cols)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.meshgrid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "border_0 = 0.7\n",
    "border_1 = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st case: 0.67, 0.12\n",
      "2nd case: 0.91, 0.09\n"
     ]
    }
   ],
   "source": [
    "n_exp = 10000\n",
    "# the target values\n",
    "y = np.zeros(15)\n",
    "y[9:] = 1\n",
    "\n",
    "accs_1 = []\n",
    "accs_2 = []\n",
    "\n",
    "for i in range(n_exp):\n",
    "  class_0 = np.random.uniform(0, border_0, size=(9, 3))\n",
    "  class_1 = np.random.uniform(border_1, 1, size=(6, 3))\n",
    "  X = np.concatenate([class_0, class_1], axis=0)\n",
    "  X[:, 1] = np.concatenate([\n",
    "    np.random.uniform(border_1, 1, size=(9, )), \n",
    "    np.random.uniform(0, border_0, size=(6, ))])\n",
    "  \n",
    "  # first case\n",
    "  y_hat = np.sum(X, axis=1)\n",
    "  # making the first 9 as the first class objects after sorting\n",
    "  # other will be as the second class\n",
    "  y_pred = np.ones(15)\n",
    "  y_pred[np.argsort(y_hat)[:9]] = 0\n",
    "\n",
    "  # accuracy\n",
    "  acc = (y_pred == y).mean()\n",
    "  accs_1.append(acc)\n",
    "\n",
    "  # second case\n",
    "  X_ = X.copy()\n",
    "  X_[:, 1] *= -1\n",
    "  y_hat = np.sum(X_, axis=1)\n",
    "  # making the first 9 as the first class objects after sorting\n",
    "  # other will be as the second class\n",
    "  y_pred = np.ones(15)\n",
    "  y_pred[np.argsort(y_hat)[:9]] = 0\n",
    "\n",
    "  # accuracy\n",
    "  acc = (y_pred == y).mean()\n",
    "  accs_2.append(acc)\n",
    "\n",
    "print(f\"1st case: {np.mean(accs_1):.2f}, {np.std(accs_1):.2f}\")\n",
    "print(f\"2nd case: {np.mean(accs_2):.2f}, {np.std(accs_2):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11 & 0.59 & 0.37 & 0.61 & 0.30 & 0.04 & 0.64 & 0.46 & 0.69 & 0.76 & 0.55 & 0.92 & 0.83 & 0.73 & 0.64\n",
      "0.85 & 0.49 & 0.68 & 0.39 & 0.58 & 0.69 & 0.98 & 0.73 & 0.61 & 0.64 & 0.65 & 0.65 & 0.70 & 0.34 & 0.07\n",
      "0.05 & 0.15 & 0.19 & 0.05 & 0.07 & 0.69 & 0.46 & 0.56 & 0.40 & 0.63 & 0.46 & 0.75 & 0.52 & 0.60 & 0.60\n"
     ]
    }
   ],
   "source": [
    "# this is one of the case since the process is randomness\n",
    "for i in range(X.shape[1]):\n",
    "  print(' & '.join([f'{val:.2f}' for val in X[:, i]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t-learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

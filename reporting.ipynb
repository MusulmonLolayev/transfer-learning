{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect and convert results to Latex tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'mobilenet_v3-cifar10.txt'.lower()\n",
    "num_exams = \"Number of examples in computing weights heuristically: \".lower()\n",
    "error_msg = \"A error occured\".lower()\n",
    "not_found = \"not found from\".lower()\n",
    "\n",
    "model_version = \"Model version:\".lower()\n",
    "\n",
    "def empty_res_obj():\n",
    "  return {\n",
    "    'subset_size': [],\n",
    "  'h_train_loss': [],\n",
    "  'h_train_acc': [],\n",
    "  'h_test_loss': [],\n",
    "  'h_test_acc': [],\n",
    "  'gh_train_loss': [],\n",
    "  'gh_train_acc': [],\n",
    "  'gh_test_loss': [],\n",
    "  'gh_test_acc': [],\n",
    "  'g_train_loss': [],\n",
    "  'g_train_acc': [],\n",
    "  'g_test_loss': [],\n",
    "  'g_test_acc': []}\n",
    "\n",
    "def extract_values(line: str):\n",
    "  arrs = line.split(' ')\n",
    "  # [:-1] dropping ','\n",
    "  return float(arrs[2][:-1]), float(arrs[4])\n",
    "\n",
    "def collect_results(path):\n",
    "  all_res = {}\n",
    "  with open(path, 'r') as f:\n",
    "    for line in f:\n",
    "      line = line.lower()\n",
    "      if model_version not in line: continue\n",
    "      \n",
    "      # get model version\n",
    "      m_v = line[line.rfind(' ')+1:-1]\n",
    "      res = empty_res_obj()\n",
    "      all_res[m_v] = res\n",
    "      for size in [64, 128, 256, 512, 1024, 2048]:\n",
    "        # skip empty line\n",
    "        f.readline()\n",
    "        # get the subset size\n",
    "        line = f.readline().lower()\n",
    "        set_size = int(line[line.rfind(' ')+1:])\n",
    "        res['subset_size'].append(set_size)\n",
    "        # skips: shape\n",
    "        f.readline()\n",
    "        # heuristic info\n",
    "        f.readline()\n",
    "\n",
    "        line = f.readline().lower()\n",
    "        # check is whether error or info\n",
    "        if error_msg in line: continue\n",
    "        line = f.readline().lower()\n",
    "        if not_found in line: continue\n",
    "\n",
    "        # get results: heuristics on train\n",
    "        loss, acc = extract_values(line)\n",
    "        res['h_train_acc'].append(acc)\n",
    "        res['h_train_loss'].append(loss)\n",
    "\n",
    "        # get results: heuristics on test\n",
    "        loss, acc = extract_values(f.readline())\n",
    "        res['h_test_acc'].append(acc)\n",
    "        res['h_test_loss'].append(loss)\n",
    "\n",
    "        # skip the next info line:\n",
    "        # Training model on all heurstic weights with epochs 5\n",
    "        f.readline()\n",
    "        # get results: train on the heuristic weights\n",
    "        line = f.readline()\n",
    "        loss, acc = extract_values(line)\n",
    "        res['gh_train_acc'].append(acc)\n",
    "        res['gh_train_loss'].append(loss)\n",
    "\n",
    "        # get results: train on the heuristic weights\n",
    "        loss, acc = extract_values(f.readline())\n",
    "        res['gh_test_acc'].append(acc)\n",
    "        res['gh_test_loss'].append(loss)\n",
    "\n",
    "        # skip the next info line:\n",
    "        # Training model on all initial weights with epochs 5\n",
    "        f.readline()\n",
    "        # get results: train on the initial weights\n",
    "        loss, acc = extract_values(f.readline())\n",
    "        res['g_train_acc'].append(acc)\n",
    "        res['g_train_loss'].append(loss)\n",
    "\n",
    "        # get results: train on the initial weights\n",
    "        loss, acc = extract_values(f.readline())\n",
    "        res['g_test_acc'].append(acc)\n",
    "        res['g_test_loss'].append(loss)\n",
    "  \n",
    "  return all_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./res-logs/visiontransformer-Adam-entropy-False-0.00(vit-b16-cl.) & 0.40/0.40 & 0.64/0.64 & 0.64/0.64 & 0.69/0.69 & 0.67/0.68 & 0.69/0.69 \\\\\n",
      "./res-logs/visiontransformer-Adam-entropy-False-0.00(vit-b16-fe.) & 0.58/0.59 & 0.66/0.66 & 0.74/0.74 & 0.72/0.72 & 0.75/0.75 & 0.76/0.76 \\\\\n",
      "./res-logs/convnext-Adam-entropy-False-0.00(base-1k-224) & 0.23/0.23 & 0.62/0.61 & 0.70/0.71 & 0.73/0.73 & 0.74/0.73 & 0.74/0.74 \\\\\n",
      "./res-logs/efficientnet-Adam-entropy-False-0.00(b0-cl.) & 0.30/0.31 & 0.35/0.35 & 0.47/0.47 & 0.50/0.50 & 0.51/0.51 & 0.50/0.50 \\\\\n",
      "./res-logs/efficientnet-Adam-entropy-False-0.00(b0-fe.) & 0.39/0.39 & 0.51/0.50 & 0.58/0.58 & 0.59/0.58 & 0.62/0.62 & 0.62/0.62 \\\\\n",
      "./res-logs/efficientnet-Adam-entropy-False-0.00(b1-cl.) & 0.24/0.25 & 0.42/0.42 & 0.49/0.48 & 0.60/0.59 & 0.60/0.59 & 0.59/0.59 \\\\\n",
      "./res-logs/efficientnet-Adam-entropy-False-0.00(b1-fe.) & 0.47/0.47 & 0.59/0.59 & 0.67/0.66 & 0.68/0.68 & 0.64/0.64 & 0.66/0.65 \\\\\n",
      "./res-logs/efficientnet-Adam-entropy-False-0.00(b4-cl.) & 0.45/0.44 & 0.51/0.51 & 0.65/0.64 & 0.68/0.68 & 0.68/0.68 & 0.69/0.68 \\\\\n",
      "./res-logs/efficientnet-Adam-entropy-False-0.00(b4-fe.) & 0.60/0.59 & 0.68/0.68 & 0.72/0.71 & 0.74/0.74 & 0.74/0.74 & 0.76/0.75 \\\\\n",
      "./res-logs/resnet50-Adam-gini-False-0.00(cl.) & 0.39/0.39 & 0.46/0.45 & 0.55/0.55 & 0.56/0.55 & 0.57/0.56 & 0.56/0.56 \\\\\n",
      "./res-logs/resnet50-Adam-gini-False-0.00(fe.) & 0.29/0.29 & 0.46/0.46 & 0.51/0.51 & 0.52/0.52 & 0.52/0.52 & 0.49/0.48 \\\\\n",
      "./res-logs/inceptionv3-Adam-gini-False-0.00(cl.) & 0.48/0.48 & 0.51/0.51 & 0.55/0.55 & 0.58/0.58 & 0.56/0.56 & 0.58/0.57 \\\\\n",
      "./res-logs/inceptionv3-Adam-gini-False-0.00(fe.) & 0.22/0.22 & 0.32/0.32 & 0.27/0.28 & 0.37/0.37 & 0.45/0.44 & 0.48/0.47 \\\\\n",
      "./res-logs/inceptionv3-Adam-entropy-False-0.00(cl.) & 0.35/0.35 & 0.42/0.42 & 0.55/0.55 & 0.57/0.57 & 0.59/0.59 & 0.60/0.59 \\\\\n",
      "./res-logs/inceptionv3-Adam-entropy-False-0.00(fe.) & 0.18/0.18 & 0.36/0.36 & 0.41/0.41 & 0.51/0.51 & 0.56/0.55 & 0.56/0.56 \\\\\n",
      "./res-logs/resnet50-nikolay-False-0.00(cl.) & 0.41/0.40 & 0.44/0.44 & 0.51/0.51 & 0.55/0.55 & 0.57/0.57 & 0.57/0.56 \\\\\n",
      "./res-logs/resnet50-nikolay-False-0.00(fe.) & 0.36/0.36 & 0.34/0.34 & 0.43/0.42 & 0.44/0.44 & 0.47/0.47 & 0.47/0.47 \\\\\n",
      "./res-logs/mobilenetv3-gini-False-0.00(large-075-224-cl.) & 0.40/0.40 & 0.48/0.48 & 0.54/0.55 & 0.60/0.61 & 0.63/0.64 & 0.63/0.63 \\\\\n",
      "./res-logs/mobilenetv3-gini-False-0.00(small-075-224-cl.) & 0.39/0.39 & 0.50/0.51 & 0.56/0.56 & 0.58/0.58 & 0.62/0.62 & 0.61/0.61 \\\\\n",
      "./res-logs/mobilenetv3-gini-False-0.00(small-075-224-fe.) & 0.49/0.49 & 0.53/0.53 & 0.60/0.61 & 0.63/0.64 & 0.64/0.64 & 0.63/0.64 \\\\\n",
      "./res-logs/visiontransformer-nikolay-False-0.00(vit-b16-cl.) & 0.41/0.42 & 0.57/0.57 & 0.63/0.63 & 0.69/0.69 & 0.68/0.68 & 0.69/0.69 \\\\\n",
      "./res-logs/visiontransformer-nikolay-False-0.00(vit-b16-fe.) & 0.64/0.64 & 0.71/0.71 & 0.76/0.76 & 0.81/0.81 & 0.82/0.82 & 0.82/0.82 \\\\\n",
      "./res-logs/efficientnet-Adam-gini-False-0.00(b0-cl.) & 0.36/0.37 & 0.48/0.48 & 0.52/0.52 & 0.50/0.51 & 0.50/0.50 & 0.51/0.52 \\\\\n",
      "./res-logs/efficientnet-Adam-gini-False-0.00(b0-fe.) & 0.43/0.43 & 0.52/0.52 & 0.58/0.57 & 0.58/0.57 & 0.60/0.60 & 0.59/0.59 \\\\\n",
      "./res-logs/efficientnet-Adam-gini-False-0.00(b1-cl.) & 0.41/0.41 & 0.55/0.55 & 0.59/0.59 & 0.58/0.58 & 0.58/0.58 & 0.59/0.59 \\\\\n",
      "./res-logs/efficientnet-Adam-gini-False-0.00(b1-fe.) & 0.45/0.46 & 0.52/0.52 & 0.56/0.56 & 0.64/0.64 & 0.63/0.63 & 0.63/0.63 \\\\\n",
      "./res-logs/efficientnet-Adam-gini-False-0.00(b4-cl.) & 0.56/0.55 & 0.60/0.59 & 0.67/0.67 & 0.68/0.68 & 0.68/0.68 & 0.68/0.68 \\\\\n",
      "./res-logs/efficientnet-Adam-gini-False-0.00(b4-fe.) & 0.66/0.65 & 0.66/0.65 & 0.70/0.69 & 0.74/0.74 & 0.73/0.73 & 0.73/0.72 \\\\\n",
      "./res-logs/mobilenetv3-Adam-entropy-False-0.00(large-075-224-cl.) & 0.48/0.49 & 0.46/0.46 & 0.55/0.55 & 0.55/0.56 & 0.56/0.56 & 0.61/0.61 \\\\\n",
      "./res-logs/mobilenetv3-Adam-entropy-False-0.00(small-075-224-cl.) & 0.43/0.43 & 0.49/0.49 & 0.49/0.49 & 0.56/0.57 & 0.58/0.57 & 0.59/0.60 \\\\\n",
      "./res-logs/mobilenetv3-Adam-entropy-False-0.00(small-075-224-fe.) & 0.46/0.46 & 0.50/0.49 & 0.56/0.56 & 0.57/0.57 & 0.62/0.62 & 0.63/0.63 \\\\\n",
      "./res-logs/mobilenetv3-nikolay-False-0.00(large-075-224-cl.) & 0.39/0.39 & 0.47/0.46 & 0.54/0.54 & 0.52/0.52 & 0.55/0.56 & 0.58/0.59 \\\\\n",
      "./res-logs/mobilenetv3-nikolay-False-0.00(small-075-224-cl.) & 0.48/0.48 & 0.56/0.55 & 0.56/0.56 & 0.57/0.57 & 0.60/0.60 & 0.61/0.62 \\\\\n",
      "./res-logs/mobilenetv3-nikolay-False-0.00(small-075-224-fe.) & 0.51/0.51 & 0.56/0.56 & 0.61/0.61 & 0.64/0.64 & 0.64/0.64 & 0.65/0.65 \\\\\n",
      "./res-logs/efficientnet-Adam-nikolay-False-0.00(b0-cl.) & 0.40/0.40 & 0.41/0.41 & 0.47/0.47 & 0.50/0.49 & 0.49/0.49 & 0.50/0.50 \\\\\n",
      "./res-logs/efficientnet-Adam-nikolay-False-0.00(b0-fe.) & 0.47/0.47 & 0.53/0.53 & 0.60/0.59 & 0.61/0.61 & 0.61/0.61 & 0.63/0.62 \\\\\n",
      "./res-logs/efficientnet-Adam-nikolay-False-0.00(b1-cl.) & 0.44/0.44 & 0.45/0.45 & 0.53/0.52 & 0.57/0.56 & 0.57/0.57 & 0.58/0.58 \\\\\n",
      "./res-logs/efficientnet-Adam-nikolay-False-0.00(b1-fe.) & 0.52/0.52 & 0.64/0.63 & 0.66/0.65 & 0.68/0.68 & 0.69/0.69 & 0.69/0.69 \\\\\n",
      "./res-logs/efficientnet-Adam-nikolay-False-0.00(b4-cl.) & 0.38/0.38 & 0.53/0.52 & 0.60/0.60 & 0.65/0.64 & 0.66/0.65 & 0.67/0.67 \\\\\n",
      "./res-logs/efficientnet-Adam-nikolay-False-0.00(b4-fe.) & 0.68/0.67 & 0.71/0.70 & 0.76/0.75 & 0.77/0.77 & 0.77/0.77 & 0.78/0.78 \\\\\n",
      "./res-logs/resnet50-Adam-entropy-False-0.00(cl.) & 0.35/0.35 & 0.41/0.41 & 0.54/0.53 & 0.53/0.53 & 0.58/0.57 & 0.58/0.57 \\\\\n",
      "./res-logs/resnet50-Adam-entropy-False-0.00(fe.) & 0.29/0.29 & 0.38/0.38 & 0.53/0.53 & 0.53/0.53 & 0.53/0.53 & 0.54/0.54 \\\\\n",
      "./res-logs/visiontransformer-Adam-gini-False-0.00(vit-b16-cl.) & 0.52/0.52 & 0.53/0.53 & 0.63/0.63 & 0.66/0.67 & 0.68/0.69 & 0.69/0.70 \\\\\n",
      "./res-logs/visiontransformer-Adam-gini-False-0.00(vit-b16-fe.) & 0.58/0.58 & 0.68/0.67 & 0.70/0.70 & 0.75/0.74 & 0.70/0.70 & 0.73/0.73 \\\\\n",
      "./res-logs/convnext-nikolay-False-0.00(base-1k-224) & 0.40/0.40 & 0.49/0.49 & 0.55/0.55 & 0.57/0.58 & 0.59/0.59 & 0.57/0.58 \\\\\n",
      "./res-logs/convnext-Adam-gini-False-0.00(base-1k-224) & 0.50/0.50 & 0.54/0.55 & 0.59/0.60 & 0.62/0.63 & 0.71/0.71 & 0.72/0.71 \\\\\n",
      "./res-logs/inceptionv3-Adam-nikolay-False-0.00(cl.) & 0.49/0.49 & 0.57/0.57 & 0.63/0.62 & 0.64/0.64 & 0.65/0.65 & 0.65/0.65 \\\\\n",
      "./res-logs/inceptionv3-Adam-nikolay-False-0.00(fe.) & 0.42/0.42 & 0.38/0.38 & 0.51/0.50 & 0.61/0.61 & 0.60/0.59 & 0.58/0.58 \\\\\n"
     ]
    }
   ],
   "source": [
    "ds_name = 'cifar10'\n",
    "all_files = glob.glob(f'./res-logs/*{ds_name}.txt')\n",
    "\n",
    "for file in all_files:\n",
    "  path = f'./{file}'\n",
    "  all_res = collect_results(path)\n",
    "  m_name = file.replace(f'{ds_name}.txt', '')[:-1]\n",
    "  m_name = m_name.replace('_', '')\n",
    "  for m_v, res in all_res.items():\n",
    "    m_v = m_v.replace('classification', 'cl.')\\\n",
    "      .replace('feature-vector', 'fe.')\\\n",
    "      .replace('-fe', '-fe.')\\\n",
    "      .replace('..', '.')\n",
    "    print(f'{m_name}({m_v})', end='')\n",
    "    for tr_acc, te_acc in zip(res['h_train_acc'], res['h_test_acc']):\n",
    "      print(f\" & {tr_acc:.2f}/{te_acc:.2f}\", end='')\n",
    "    print(' \\\\\\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "border_0 = 0.7\n",
    "border_1 = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st case: 0.67, 0.12\n",
      "2nd case: 0.91, 0.09\n"
     ]
    }
   ],
   "source": [
    "n_exp = 10000\n",
    "# the target values\n",
    "y = np.zeros(15)\n",
    "y[9:] = 1\n",
    "\n",
    "accs_1 = []\n",
    "accs_2 = []\n",
    "\n",
    "for i in range(n_exp):\n",
    "  class_0 = np.random.uniform(0, border_0, size=(9, 3))\n",
    "  class_1 = np.random.uniform(border_1, 1, size=(6, 3))\n",
    "  X = np.concatenate([class_0, class_1], axis=0)\n",
    "  X[:, 1] = np.concatenate([\n",
    "    np.random.uniform(border_1, 1, size=(9, )), \n",
    "    np.random.uniform(0, border_0, size=(6, ))])\n",
    "  \n",
    "  # first case\n",
    "  y_hat = np.sum(X, axis=1)\n",
    "  # making the first 9 as the first class objects after sorting\n",
    "  # other will be as the second class\n",
    "  y_pred = np.ones(15)\n",
    "  y_pred[np.argsort(y_hat)[:9]] = 0\n",
    "\n",
    "  # accuracy\n",
    "  acc = (y_pred == y).mean()\n",
    "  accs_1.append(acc)\n",
    "\n",
    "  # second case\n",
    "  X_ = X.copy()\n",
    "  X_[:, 1] *= -1\n",
    "  y_hat = np.sum(X_, axis=1)\n",
    "  # making the first 9 as the first class objects after sorting\n",
    "  # other will be as the second class\n",
    "  y_pred = np.ones(15)\n",
    "  y_pred[np.argsort(y_hat)[:9]] = 0\n",
    "\n",
    "  # accuracy\n",
    "  acc = (y_pred == y).mean()\n",
    "  accs_2.append(acc)\n",
    "\n",
    "print(f\"1st case: {np.mean(accs_1):.2f}, {np.std(accs_1):.2f}\")\n",
    "print(f\"2nd case: {np.mean(accs_2):.2f}, {np.std(accs_2):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24 & 0.59 & 0.26 & 0.42 & 0.36 & 0.25 & 0.53 & 0.45 & 0.22 & 0.51 & 0.56 & 0.90 & 0.51 & 0.62 & 0.99\n",
      "0.32 & 0.67 & 0.46 & 0.46 & 0.45 & 0.54 & 0.46 & 0.99 & 0.34 & 0.10 & 0.62 & 0.38 & 0.32 & 0.07 & 0.46\n",
      "0.67 & 0.01 & 0.46 & 0.24 & 0.70 & 0.34 & 0.35 & 0.11 & 0.55 & 0.57 & 0.86 & 0.46 & 0.42 & 0.78 & 0.41\n"
     ]
    }
   ],
   "source": [
    "# this is one of the case since the process is randomness\n",
    "for i in range(X.shape[1]):\n",
    "  print(' & '.join([f'{val:.2f}' for val in X[:, i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The second figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.use('PS')\n",
    "mpl.rcParams['text.usetex'] = True\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_types = ['nikolay',  'entropy', 'gini']\n",
    "markers = {'nikolay': '*', 'entropy': 'x', 'gini': '1'}\n",
    "colors = {'nikolay': 'red', 'entropy': 'black', 'gini': 'yellow'}\n",
    "for w_type in w_types:\n",
    "  all_files = glob.glob(f'./res-logs/*{w_type}*.txt')\n",
    "  all_files = sorted(all_files)\n",
    "  j = 0\n",
    "  for file in all_files:\n",
    "    j += 1\n",
    "    if j < 6:\n",
    "      continue\n",
    "    path = f'./{file}'\n",
    "    all_res = collect_results(path)\n",
    "    # over inner dict\n",
    "    i = 0\n",
    "    for model_type in all_res:\n",
    "      i += 1\n",
    "      subset_size = all_res[model_type]['subset_size']\n",
    "      subset_size = [str(size) for size in subset_size]\n",
    "      h_test_acc = all_res[model_type]['h_test_acc']\n",
    "      plt.plot(subset_size, h_test_acc, marker=markers[w_type])\n",
    "      # if i > 1:\n",
    "      break\n",
    "    if j > 20:\n",
    "      break\n",
    "  break\n",
    "\n",
    "plt.ylim((0, 1))\n",
    "plt.xticks(('64', '128', '256', '512', '1024', '2048'))\n",
    "plt.show()\n",
    "plt.savefig('2th.eps', format='eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The third figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_types = ['nikolay',  'entropy', 'gini']\n",
    "\n",
    "res_files = {w_type: {file_name.replace(f'{w_type}-False-0.00-', '').replace('-Adam', ''): file_name for file_name in glob.glob(f'./res-logs/*{w_type}*.txt')} for w_type in w_types}\n",
    "\n",
    "stats = {\n",
    "  'nikolay': np.zeros(6),\n",
    "  'entropy': np.zeros(6),\n",
    "  'gini': np.zeros(6),\n",
    "}\n",
    "subset_sizes = [64, 128, 256, 512, 1024, 2048]\n",
    "l = 0\n",
    "for nik_key, ent_key, gini_key in zip(sorted(res_files['nikolay'].keys()),\n",
    "                                      sorted(res_files['entropy'].keys()),\n",
    "                                      sorted(res_files['gini'].keys())):\n",
    "  path_nik = './' + res_files['nikolay'][nik_key]\n",
    "  path_ent = './' + res_files['entropy'][ent_key]\n",
    "  path_gini = './' + res_files['gini'][gini_key]\n",
    "\n",
    "  res_nik = collect_results(path_nik)\n",
    "  res_ent = collect_results(path_ent)\n",
    "  res_gini = collect_results(path_gini)\n",
    "\n",
    "  for item_nik, item_ent, item_gini in zip(res_nik.values(),\n",
    "                                           res_ent.values(),\n",
    "                                           res_gini.values()):\n",
    "    for i in range(6):\n",
    "      l += 1\n",
    "      nik_acc = item_nik['h_test_acc'][i]\n",
    "      ent_acc = item_ent['h_test_acc'][i]\n",
    "      gini_acc = item_gini['h_test_acc'][i]\n",
    "      array = np.array([nik_acc, ent_acc, gini_acc])\n",
    "      stats[w_types[array.argmax()]][i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nikolay': array([76., 66., 47., 57., 55., 56.]),\n",
       " 'entropy': array([31., 35., 54., 50., 48., 41.]),\n",
       " 'gini': array([38., 44., 44., 38., 42., 48.])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(subset_sizes))  # the label locations\n",
    "width = 0.25  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(layout='constrained')\n",
    "\n",
    "for attribute, measurement in stats.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
    "    ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Number of outperformes')\n",
    "ax.set_xlabel('Subset sizes')\n",
    "ax.set_xticks(x + width, subset_sizes)\n",
    "ax.legend(loc='upper center', ncols=3)\n",
    "ax.set_ylim(0, 80)\n",
    "\n",
    "plt.savefig('3th.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variances of accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_types = ['nikolay',  'entropy', 'gini']\n",
    "\n",
    "res_files = {w_type: {file_name.replace(f'{w_type}-False-0.00-', '').replace('-Adam', ''): file_name for file_name in glob.glob(f'./res-logs/*{w_type}*.txt')} for w_type in w_types}\n",
    "\n",
    "vars = np.zeros((6, 3, 3))\n",
    "subset_sizes = [64, 128, 256, 512, 1024, 2048]\n",
    "for nik_key, ent_key, gini_key in zip(sorted(res_files['nikolay'].keys()),\n",
    "                                      sorted(res_files['entropy'].keys()),\n",
    "                                      sorted(res_files['gini'].keys())):\n",
    "  path_nik = './' + res_files['nikolay'][nik_key]\n",
    "  path_ent = './' + res_files['entropy'][ent_key]\n",
    "  path_gini = './' + res_files['gini'][gini_key]\n",
    "\n",
    "  res_nik = collect_results(path_nik)\n",
    "  res_ent = collect_results(path_ent)\n",
    "  res_gini = collect_results(path_gini)\n",
    "\n",
    "  for item_nik, item_ent, item_gini in zip(res_nik.values(),\n",
    "                                           res_ent.values(),\n",
    "                                           res_gini.values()):\n",
    "    for i in range(6):\n",
    "      nik_acc = item_nik['h_test_acc'][i]\n",
    "      ent_acc = item_ent['h_test_acc'][i]\n",
    "      gini_acc = item_gini['h_test_acc'][i]\n",
    "      vars[i, 0, 1] += abs(nik_acc - ent_acc)\n",
    "      vars[i, 0, 2] += abs(nik_acc - gini_acc)\n",
    "      vars[i, 1, 2] += abs(ent_acc - gini_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 4.86789655, 4.6172069 ],\n",
       "       [0.        , 0.        , 2.68055172],\n",
       "       [0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(vars / 145, axis=0) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained models details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 17:01:33.435467: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-17 17:01:33.461389: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-17 17:01:33.864129: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from utils import img_model_links, text_model_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_bert & \\href{https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1}{small_bert/bert_en_uncased_L-2_H-128_A-2} \\\\\n",
      "small_bert & \\href{https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1}{small_bert/bert_en_uncased_L-2_H-256_A-4} \\\\\n",
      "small_bert & \\href{https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1}{small_bert/bert_en_uncased_L-2_H-512_A-8} \\\\\n",
      "small_bert & \\href{https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1}{small_bert/bert_en_uncased_L-4_H-512_A-8} \\\\\n",
      "bert & \\href{https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3}{bert_en_uncased_L-12_H-768_A-12} \\\\\n",
      "bert & \\href{https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3}{bert_en_cased_L-12_H-768_A-12} \\\\\n",
      "bert & \\href{https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3}{bert_multi_cased_L-12_H-768_A-12} \\\\\n",
      "albert & \\href{https://tfhub.dev/tensorflow/albert_en_base/2}{albert_en_base} \\\\\n",
      "electra & \\href{https://tfhub.dev/google/electra_small/2}{electra_small} \\\\\n",
      "electra & \\href{https://tfhub.dev/google/electra_base/2}{electra_base} \\\\\n",
      "experts & \\href{https://tfhub.dev/google/experts/bert/pubmed/2}{experts_pubmed} \\\\\n",
      "experts & \\href{https://tfhub.dev/google/experts/bert/wiki_books/2}{experts_wiki_books} \\\\\n",
      "talking-heads & \\href{https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1}{talking-heads_base} \\\\\n"
     ]
    }
   ],
   "source": [
    "for key in text_model_links:\n",
    "  for version, link in text_model_links[key].items():\n",
    "    print(key + \" & \\href{\" + link + \"}{\" + version + \"} \\\\\\\\\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t-learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
